# Mixtral Optimization Guide for nano-vLLM

å½“å‰ Mixtral 8x7B æ¨¡å‹è¿è¡Œé€Ÿåº¦ææ…¢ï¼ˆå›ç­”ä¸€ä¸ªé—®é¢˜éœ€è¦2å°æ—¶ï¼‰çš„æ€§èƒ½ä¼˜åŒ–æŒ‡å—

## é—®é¢˜åˆ†æ

åŸºäºå¯¹ä»£ç çš„æ·±å…¥åˆ†æï¼Œå‘ç°äº†ä»¥ä¸‹ä¸»è¦æ€§èƒ½ç“¶é¢ˆï¼š

### 1. åŠ¨æ€ä¸“å®¶åŠ è½½æœºåˆ¶ (ä¸»è¦ç“¶é¢ˆ)

**é—®é¢˜æè¿°:**
- ä¸“å®¶æƒé‡åœ¨è¿è¡Œæ—¶åŠ¨æ€ä»ç£ç›˜/CPUåŠ è½½åˆ°GPU
- æ¯æ¬¡forward passå¯èƒ½è§¦å‘å¤šæ¬¡ä¸“å®¶åŠ è½½
- LRUç¼“å­˜å®¹é‡æœ‰é™ï¼ˆé»˜è®¤42ä¸ªä¸“å®¶ï¼‰ï¼Œé¢‘ç¹å‘ç”Ÿç¼“å­˜ç¼ºå¤±

**å½±å“ç¨‹åº¦:** ğŸ”´ ä¸¥é‡ - å¯èƒ½å¯¼è‡´æ•°ç™¾å€çš„æ€§èƒ½ä¸‹é™

**ä»£ç ä½ç½®:**
```python
# nanovllm/engine/expert_manager.py:59-100
def get_expert(self, layer_idx: int, expert_idx: int) -> MixtralExpert:
    # ç¼“å­˜ç¼ºå¤±æ—¶éœ€è¦ä»ç£ç›˜åŠ è½½ä¸“å®¶æƒé‡
    expert_weights = load_expert_weights(...)  # ç£ç›˜I/Oç“¶é¢ˆ
```

### 2. ä¸²è¡Œä¸“å®¶å¤„ç†

**é—®é¢˜æè¿°:**
- ä¸“å®¶åœ¨forå¾ªç¯ä¸­é€ä¸ªå¤„ç†ï¼Œæ— å¹¶è¡ŒåŒ–
- æ¯ä¸ªä¸“å®¶å•ç‹¬è¿›è¡Œå‰å‘ä¼ æ’­

**å½±å“ç¨‹åº¦:** ğŸŸ¡ ä¸­ç­‰ - çº¿æ€§å¢åŠ è®¡ç®—æ—¶é—´

**ä»£ç ä½ç½®:**
```python
# nanovllm/models/mixtral.py:138-171  
for expert_idx in range(self.num_experts):  # ä¸²è¡Œå¤„ç†8ä¸ªä¸“å®¶
    expert = expert_manager.get_expert(self.layer_idx, expert_idx)
    expert_output = expert(expert_input)
```

### 3. ä½æ•ˆçš„è·¯ç”±æƒé‡åº”ç”¨

**é—®é¢˜æè¿°:**
- åµŒå¥—å¾ªç¯åº”ç”¨è·¯ç”±æƒé‡
- æ¯ä¸ªtoken-expertç»„åˆå•ç‹¬å¤„ç†

**å½±å“ç¨‹åº¦:** ğŸŸ¡ ä¸­ç­‰ - éšbatch sizeå’Œä¸“å®¶æ•°é‡å¢é•¿

### 4. å†…å­˜ç®¡ç†ä½æ•ˆ

**é—®é¢˜æè¿°:**
- æ¯æ¬¡forward passåˆ›å»ºæ–°çš„è¾“å‡ºå¼ é‡
- ä¸“å®¶åŠ è½½æ—¶çš„å†…å­˜ç¢ç‰‡åŒ–

**å½±å“ç¨‹åº¦:** ğŸŸ¡ ä¸­ç­‰ - å¢åŠ å†…å­˜åˆ†é…å¼€é”€

## ä¼˜åŒ–æ–¹æ¡ˆ

### ç«‹å³å¯å®æ–½çš„ä¼˜åŒ– (Quick Wins)



### ä¸­æœŸä¼˜åŒ– (éœ€è¦ä»£ç ä¿®æ”¹)

#### 1. å®ç°æ‰¹é‡ä¸“å®¶å¤„ç†

**ç›®æ ‡æ–‡ä»¶:** `nanovllm/models/mixtral.py:138-171`

```python
# æ›¿æ¢ä¸²è¡Œå¤„ç†ä¸ºæ‰¹é‡å¤„ç†
def forward_batch_experts(self, hidden_states, selected_experts, routing_weights):
    # æŒ‰ä¸“å®¶åˆ†ç»„token
    expert_tokens = {}
    for token_idx, experts in enumerate(selected_experts):
        for pos, expert_idx in enumerate(experts):
            if expert_idx not in expert_tokens:
                expert_tokens[expert_idx] = []
            expert_tokens[expert_idx].append((token_idx, pos))
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰ä¸“å®¶
    expert_outputs = {}
    for expert_idx, token_list in expert_tokens.items():
        token_indices = [t[0] for t in token_list]
        expert_input = hidden_states[token_indices]
        expert = self.get_expert(expert_idx)  # ä¸€æ¬¡æ€§è·å–
        expert_outputs[expert_idx] = expert(expert_input)  # æ‰¹é‡å¤„ç†
```

#### 2. å‘é‡åŒ–æƒé‡åº”ç”¨

```python
# ä½¿ç”¨torch.index_add_è¿›è¡Œå‘é‡åŒ–æƒé‡åº”ç”¨
def apply_expert_weights_vectorized(self, final_output, expert_outputs, routing_weights):
    for expert_idx, outputs in expert_outputs.items():
        weights = routing_weights[:, expert_idx].unsqueeze(-1)  # å¹¿æ’­æƒé‡
        final_output.index_add_(0, token_indices, weights * outputs)
```

#### 3. å†…å­˜æ± ä¼˜åŒ–

```python
class MixtralLayer:
    def __init__(self):
        # é¢„åˆ†é…å¼ é‡æ± 
        self.tensor_pool = TensorPool()
        
    def forward(self, hidden_states):
        final_output = self.tensor_pool.get_tensor(hidden_states.shape)
        # ... å¤„ç†é€»è¾‘
        return final_output
```

### é•¿æœŸä¼˜åŒ– (æ¶æ„å±‚é¢)

#### 1. å®ç°ä¸“å®¶å¹¶è¡ŒåŒ–

**å‚è€ƒä»£ç :** `nanovllm/layers/moe.py`ä¸­çš„`FusedMoE`å®ç°

- å°†ä¸“å®¶æƒé‡åˆ†ç‰‡åˆ°å¤šä¸ªGPU
- ä½¿ç”¨tensor parallelè¿›è¡Œä¸“å®¶è®¡ç®—
- å®ç°all-reduceèšåˆç»“æœ

#### 2. é‡åŒ–å’Œç¨€ç–åŒ–

- å®ç°INT8/FP16ä¸“å®¶é‡åŒ–
- ä½¿ç”¨ç»“æ„åŒ–ç¨€ç–å‡å°‘è®¡ç®—é‡
- åŠ¨æ€å‰ªæä¸æ´»è·ƒçš„ä¸“å®¶

#### 3. ä¸“å®¶ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

```python
class SmartExpertManager:
    def __init__(self):
        self.usage_predictor = ExpertUsagePredictor()
        self.cache_policy = AdaptiveLRU()
    
    def predict_and_preload(self, context):
        likely_experts = self.usage_predictor.predict(context)
        self.preload_experts(likely_experts)
```

## å®æ–½ä¼˜å…ˆçº§

### P0 - ç«‹å³å®æ–½ (é¢„è®¡æé€Ÿ10-50å€)
1. âœ… è®¾ç½®`max_gpu_experts=224`é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶
2. âœ… è°ƒæ•´`gpu_memory_utilization=0.95`
3. âœ… å‡å°‘`max_num_batched_tokens=1024`å’Œ`max_model_len=512`

### P1 - æœ¬å‘¨å†…å®æ–½ (é¢„è®¡é¢å¤–æé€Ÿ2-5å€)  
1. å®ç°ä¸“å®¶é¢„çƒ­æœºåˆ¶
2. ä¼˜åŒ–ExpertManagerçš„ç¼“å­˜ç­–ç•¥
3. å‘é‡åŒ–æƒé‡åº”ç”¨é€»è¾‘

### P2 - æœ¬æœˆå†…å®æ–½ (é¢„è®¡é¢å¤–æé€Ÿ2-3å€)
1. å®ç°æ‰¹é‡ä¸“å®¶å¤„ç†
2. å†…å­˜æ± å’Œå¼ é‡é‡ç”¨
3. ä¸“å®¶é‡åŒ–æ”¯æŒ

## æ€§èƒ½æµ‹è¯•å»ºè®®

### åŸºå‡†æµ‹è¯•è®¾ç½®
```python
# æµ‹è¯•è„šæœ¬é…ç½®
TEST_PROMPTS = [
    "çŸ­æ–‡æœ¬æµ‹è¯• (10 tokens)",
    "ä¸­ç­‰æ–‡æœ¬æµ‹è¯•ï¼ŒåŒ…å«æ›´å¤æ‚çš„å†…å®¹å’Œå¤šä¸ªå¥å­ (50 tokens)",  
    "é•¿æ–‡æœ¬æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®åº”ç”¨åœºæ™¯ï¼ŒåŒ…å«å¤æ‚æ¨ç†å’Œå¤šè½®å¯¹è¯å†…å®¹ (200 tokens)"
]

METRICS = [
    "é¦–æ¬¡æ¨ç†å»¶è¿Ÿ (åŒ…å«æ¨¡å‹åŠ è½½)",
    "åç»­æ¨ç†å»¶è¿Ÿ (æ’é™¤åŠ è½½)",  
    "Tokenç”Ÿæˆé€Ÿåº¦ (tokens/sec)",
    "GPUå†…å­˜ä½¿ç”¨å³°å€¼",
    "ä¸“å®¶ç¼“å­˜å‘½ä¸­ç‡"
]
```

### é¢„æœŸæ€§èƒ½ç›®æ ‡

| ä¼˜åŒ–é˜¶æ®µ | Tokenç”Ÿæˆé€Ÿåº¦ | å†…å­˜ä½¿ç”¨ | é¦–æ¬¡å»¶è¿Ÿ |
|---------|-------------|----------|---------|
| å½“å‰ | 0.01 tokens/s | 22GB | 2å°æ—¶ |
| P0ä¼˜åŒ–å | 1-5 tokens/s | 22GB | 5-30åˆ†é’Ÿ |
| P1ä¼˜åŒ–å | 5-15 tokens/s | 20GB | 1-5åˆ†é’Ÿ |  
| P2ä¼˜åŒ–å | 15-30 tokens/s | 18GB | <1åˆ†é’Ÿ |

## ç›‘æ§å’Œè°ƒè¯•

### å…³é”®æ€§èƒ½æŒ‡æ ‡
```python
# åœ¨ModelRunnerä¸­æ·»åŠ æ€§èƒ½ç›‘æ§
class PerformanceMonitor:
    def log_expert_stats(self):
        print(f"Expert cache hit rate: {self.hit_rate:.1%}")
        print(f"Average expert load time: {self.avg_load_time:.3f}s")
        print(f"Memory usage: {torch.cuda.memory_allocated()/1024**3:.1f}GB")
```

### è°ƒè¯•å‘½ä»¤
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export NANOVLLM_LOG_LEVEL=DEBUG

# ç›‘æ§GPUå†…å­˜
watch -n 1 nvidia-smi

# åˆ†æä¸“å®¶ä½¿ç”¨æ¨¡å¼  
python scripts/analyze_expert_usage.py
```

## æ€»ç»“

é€šè¿‡å®æ–½P0ä¼˜åŒ–ï¼Œé¢„è®¡å¯ä»¥å°†å½“å‰2å°æ—¶çš„å“åº”æ—¶é—´é™ä½åˆ°5-30åˆ†é’Ÿï¼ŒåŸºæœ¬è¾¾åˆ°å¯ç”¨æ°´å¹³ã€‚åç»­çš„P1å’ŒP2ä¼˜åŒ–å°†è¿›ä¸€æ­¥æå‡æ€§èƒ½åˆ°æ¥è¿‘vLLMçš„æ°´å¹³ã€‚

å…³é”®æ˜¯è¦å…ˆè§£å†³åŠ¨æ€ä¸“å®¶åŠ è½½è¿™ä¸ªæœ€å¤§çš„ç“¶é¢ˆï¼Œç„¶åé€æ­¥ä¼˜åŒ–è®¡ç®—å’Œå†…å­˜æ•ˆç‡ã€‚