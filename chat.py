#!/usr/bin/env python3
"""
æµå¼èŠå¤©ç¨‹åº - æ”¯æŒå®æ—¶å•è¯è¾“å‡º
"""

import os
import sys
import time
from nanovllm import LLM, SamplingParams
from transformers import AutoTokenizer


class StreamingChat:
    def __init__(self, model_path="/home/asu/qwen3-0.6b", typing_effect=True, system_prompt=None):
        """åˆå§‹åŒ–èŠå¤©ç³»ç»Ÿ"""
        print("ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
        
        self.model_path = os.path.expanduser(model_path)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.llm = LLM(self.model_path, enforce_eager=True, tensor_parallel_size=1)
        self.typing_effect = typing_effect
        
        # è®¾ç½®system prompt
        if system_prompt is None:
            self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨ã€æ— å®³ã€è¯šå®çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚"
        else:
            self.system_prompt = system_prompt
        
        # è®¾ç½®é‡‡æ ·å‚æ•°
        self.sampling_params = SamplingParams(
            temperature=0.7,
            max_tokens=512,
            ignore_eos=False
        )
        
        effect_status = "å¼€å¯" if typing_effect else "å…³é—­"
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼æ‰“å­—æ•ˆæœ: {effect_status}")
        print(f"ğŸ¯ System Prompt: {self.system_prompt[:50]}...")
        print()
    
    def format_prompt(self, user_input):
        """æ ¼å¼åŒ–ç”¨æˆ·è¾“å…¥ä¸ºèŠå¤©æ¨¡æ¿"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_input}
        ]
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=True
        )
    
    def stream_generate(self, prompt):
        """æµå¼ç”Ÿæˆå›å¤"""
        # æ·»åŠ è¯·æ±‚åˆ°å¼•æ“
        self.llm.add_request(prompt, self.sampling_params)
        
        # ç”¨äºè¿½è¸ªå·²è¾“å‡ºçš„å†…å®¹
        last_decoded_length = 0
        sequence_id = None
        
        print("ğŸ¤– AI: ", end="", flush=True)
        
        # å¾ªç¯ç”Ÿæˆç›´åˆ°å®Œæˆ
        while not self.llm.is_finished():
            # æ‰§è¡Œä¸€æ­¥ç”Ÿæˆ
            outputs, _ = self.llm.step()
            
            # å¦‚æœæœ‰åºåˆ—å®Œæˆäº†ï¼Œè¾“å‡ºæœ€ç»ˆç»“æœ
            for seq_id, token_ids in outputs:
                if token_ids:
                    # è§£ç å®Œæ•´æ–‡æœ¬
                    full_text = self.tokenizer.decode(token_ids)
                    # è¾“å‡ºå‰©ä½™æœªè¾“å‡ºçš„éƒ¨åˆ†
                    remaining_text = full_text[last_decoded_length:]
                    if remaining_text:
                        print(remaining_text, end="", flush=True)
                    return  # åºåˆ—å®Œæˆï¼Œé€€å‡º
            
            # æ£€æŸ¥æ­£åœ¨è¿è¡Œçš„åºåˆ—ï¼Œè¾“å‡ºæ–°ç”Ÿæˆçš„å†…å®¹
            for seq in self.llm.scheduler.running:
                if sequence_id is None:
                    sequence_id = seq.seq_id
                
                if seq.seq_id == sequence_id and seq.num_completion_tokens > 0:
                    # è§£ç å½“å‰æ‰€æœ‰completion tokens
                    current_text = self.tokenizer.decode(seq.completion_token_ids)
                    
                                         # åªè¾“å‡ºæ–°å¢çš„éƒ¨åˆ†
                    if len(current_text) > last_decoded_length:
                        new_text = current_text[last_decoded_length:]
                        
                        if self.typing_effect:
                            # æŒ‰å­—ç¬¦é€ä¸ªè¾“å‡ºï¼Œè¥é€ æ‰“å­—æ•ˆæœ
                            for char in new_text:
                                print(char, end="", flush=True)
                                # æ·»åŠ ä¸åŒå»¶è¿Ÿæ¥æ¨¡æ‹ŸçœŸå®æ‰“å­—
                                if char in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,!?;:':
                                    time.sleep(0.2)  # æ ‡ç‚¹ç¬¦å·åç¨å¾®æš‚åœé•¿ä¸€ç‚¹
                                elif char == ' ':
                                    time.sleep(0.1)  # ç©ºæ ¼åæš‚åœç¨é•¿
                                else:
                                    time.sleep(0.05)  # æ™®é€šå­—ç¬¦
                        else:
                            # ç›´æ¥è¾“å‡ºæ–°æ–‡æœ¬ï¼Œåªæ·»åŠ å¾ˆçŸ­çš„å»¶è¿Ÿ
                            print(new_text, end="", flush=True)
                            time.sleep(0.01)
                        
                        last_decoded_length = len(current_text)
                    break
        
        print("\n")  # æ¢è¡Œç»“æŸå›å¤
    
    def chat_loop(self):
        """ä¸»èŠå¤©å¾ªç¯"""
        print("ğŸ’¬ æ¬¢è¿ä½¿ç”¨æµå¼èŠå¤©ç¨‹åºï¼")
        print("ğŸ“ è¾“å…¥ä½ çš„é—®é¢˜ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº")
        print("âš™ï¸  è¾“å…¥ 'toggle' åˆ‡æ¢æ‰“å­—æ•ˆæœå¼€å…³")
        print("-" * 50)
        
        while True:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            
            # æ£€æŸ¥é€€å‡ºå‘½ä»¤
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            # æ£€æŸ¥åˆ‡æ¢æ‰“å­—æ•ˆæœå‘½ä»¤
            if user_input.lower() in ['toggle', 'switch', 'åˆ‡æ¢']:
                self.typing_effect = not self.typing_effect
                status = "å¼€å¯" if self.typing_effect else "å…³é—­"
                print(f"âš™ï¸  æ‰“å­—æ•ˆæœå·²{status}")
                continue
            
            # æ£€æŸ¥ç©ºè¾“å…¥
            if not user_input:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜")
                continue
            
            # æ ¼å¼åŒ–promptå¹¶ç”Ÿæˆå›å¤
            formatted_prompt = self.format_prompt(user_input)
            self.stream_generate(formatted_prompt)


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    default_model_path = "/home/asu/qwen3-0.6b"
    model_path = default_model_path
    typing_effect = True
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]
        if arg in ['--no-typing', '--fast']:
            typing_effect = False
        elif arg in ['--typing', '--slow']:
            typing_effect = True
        elif arg in ['--help', '-h']:
            print("æµå¼èŠå¤©ç¨‹åº")
            print("ç”¨æ³•: python chat.py [æ¨¡å‹è·¯å¾„] [é€‰é¡¹]")
            print("é€‰é¡¹:")
            print("  --typing, --slow    å¯ç”¨æ‰“å­—æ•ˆæœ (é»˜è®¤)")
            print("  --no-typing, --fast ç¦ç”¨æ‰“å­—æ•ˆæœ")
            print("  --help, -h          æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
            print("ç¤ºä¾‹:")
            print("  python chat.py                           # ä½¿ç”¨é»˜è®¤æ¨¡å‹å’Œæ‰“å­—æ•ˆæœ")
            print("  python chat.py --no-typing               # ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼Œç¦ç”¨æ‰“å­—æ•ˆæœ")
            print("  python chat.py /path/to/model --fast     # ä½¿ç”¨æŒ‡å®šæ¨¡å‹ï¼Œç¦ç”¨æ‰“å­—æ•ˆæœ")
            sys.exit(0)
        elif not arg.startswith('--'):
            model_path = arg
        i += 1
    
    if not os.path.exists(os.path.expanduser(model_path)):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾„ {model_path}")
        print(f"ğŸ’¡ è¯·ç¡®è®¤æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œæˆ–è€…æä¾›æ­£ç¡®çš„è·¯å¾„ä½œä¸ºå‚æ•°:")
        print(f"   python chat.py /path/to/your/model")
        print(f"ğŸ’¡ ä½¿ç”¨ --help æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹")
        sys.exit(1)
    
    # åˆ›å»ºå¹¶å¯åŠ¨èŠå¤©
    chat = StreamingChat(model_path, typing_effect)
    try:
        chat.chat_loop()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œç¨‹åºé€€å‡º")


if __name__ == "__main__":
    main() 