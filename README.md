# Nano-vLLM

A lightweight vLLM implementation built from scratch.

## Key Features

* ğŸš€ **Fast offline inference** - Comparable inference speeds to vLLM
* ğŸ“– **Readable codebase** - Clean implementation in ~ 1,200 lines of Python code
* âš¡ **Optimization Suite** - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.
* ğŸ¯ **MoE Support** - Mixtral 8x7B with dynamic expert loading
* ğŸ”§ **Multi-GPU Architecture Support** - Flash Attention 1.x for Turing (2080 Ti), 2.x for Ampere+

## Installation

```bash
pip install git+https://github.com/GeeeekExplorer/nano-vllm.git
```

## Manual Download

If you prefer to download the model weights manually, use the following command:
```bash
huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False
```

## Quick Start

See `example.py` for usage. The API mirrors vLLM's interface with minor differences in the `LLM.generate` method:
```python
from nanovllm import LLM, SamplingParams
llm = LLM("/YOUR/MODEL/PATH", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]
```

---

## ğŸ¯ åœ¨ RTX 2080 Ti ä¸Šè¿è¡Œ Mixtral 8x7B

### 1. ç¯å¢ƒå‡†å¤‡

```bash
cd /home/asu/Desktop/nano-vllm

# åˆ›å»º2080 Tiä¸“ç”¨ç¯å¢ƒ
cd envs/2080ti
uv sync

# å®‰è£… Flash Attention 1.x (æ”¯æŒTuringæ¶æ„)
uv pip install flash-attn==1.0.9 --no-build-isolation -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 2. ä¸‹è½½ Mixtral æ¨¡å‹

```bash
# ä½¿ç”¨ huggingface-cli
huggingface-cli download --resume-download mistralai/Mixtral-8x7B-v0.1 \
  --local-dir ./Mixtral-8x7B-v0.1/ \
  --local-dir-use-symlinks False

# æˆ–ä½¿ç”¨ hfd.sh (æ¨èï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ )
./hfd.sh mistralai/Mixtral-8x7B-v0.1 --local-dir ./Mixtral-8x7B-v0.1/
```

### 3. è¿è¡Œ Mixtral

```bash
# æŒ‡å®šä½¿ç”¨ 2080 Ti (PyTorch GPU 1)
CUDA_VISIBLE_DEVICES=1 uv run python example_mixtral.py
```

æˆ–è€…ä½¿ç”¨Python APIï¼š

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # é€‰æ‹©2080 Ti

from nanovllm.config import Config
from nanovllm.engine.model_runner import ModelRunner

# é…ç½®
config = Config(
    model="/path/to/Mixtral-8x7B-v0.1",
    tensor_parallel_size=1,
    max_num_batched_tokens=2048,
    max_model_len=1024,
    gpu_memory_utilization=0.85,
    enforce_eager=True,  # 2080 Tiå»ºè®®å¼€å¯
)

# åˆå§‹åŒ–
runner = ModelRunner(config, rank=0, event=None)

# ç”Ÿæˆ...
```

### 4. æ€§èƒ½ä¼˜åŒ–å»ºè®®

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `max_model_len` | 512-1024 | å‡å°‘KV cacheå ç”¨ |
| `max_num_batched_tokens` | 1024-2048 | æ§åˆ¶æ˜¾å­˜å³°å€¼ |
| `gpu_memory_utilization` | 0.85 | ç•™å‡ºç©ºé—´ç»™æ¿€æ´»å€¼ |
| `enforce_eager` | True | 2080 Tiå»ºè®®ç¦ç”¨CUDA Graph |

### 5. éªŒè¯ Flash Attention

```bash
CUDA_VISIBLE_DEVICES=1 uv run python -c "
from nanovllm.layers.attention import print_attention_backend_info
print_attention_backend_info()
"
```

é¢„æœŸè¾“å‡ºï¼š
```
GPU: NVIDIA GeForce RTX 2080 Ti (compute 7.5)
GPUæ¶æ„: Turing (RTX 20ç³»åˆ—, æ”¯æŒFlash Attn 1.x)
Flash Attention 1.x: âœ“ å¯ç”¨
```

### 6. Attention æ€§èƒ½å¯¹æ¯”

åœ¨ RTX 2080 Ti ä¸Šçš„ Prefill æ€§èƒ½ï¼š

| seq_len | PyTorch SDPA | Flash Attn 1.x | æå‡ |
|---------|-------------|----------------|------|
| 128 | 0.77ms | 0.16ms | **4.8x** |
| 256 | 0.73ms | 0.17ms | **4.3x** |
| 512 | 0.81ms | 0.35ms | **2.3x** |
| 1024 | 1.10ms | 0.80ms | **1.4x** |

---

## Benchmark

See `bench.py` for benchmark.

**Test Configuration:**
- Hardware: RTX 4070 Laptop (8GB)
- Model: Qwen3-0.6B
- Total Requests: 256 sequences
- Input Length: Randomly sampled between 100â€“1024 tokens
- Output Length: Randomly sampled between 100â€“1024 tokens

**Performance Results:**
| Inference Engine | Output Tokens | Time (s) | Throughput (tokens/s) |
|----------------|-------------|----------|-----------------------|
| vLLM           | 133,966     | 98.37    | 1361.84               |
| Nano-vLLM      | 133,966     | 93.41    | 1434.13               |


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=GeeeekExplorer/nano-vllm&type=Date)](https://www.star-history.com/#GeeeekExplorer/nano-vllm&Date)


## SVD

```
CUDA_VISIBLE_DEVICES=1 uv run python scripts/decompose_experts.py \
    --model-path /home/asu/Desktop/nano-vllm/Mixtral-8x7B-v0.1 \
    --rank 256

```


```

CUDA_VISIBLE_DEVICES=1 uv run python example_mixtral.py

```