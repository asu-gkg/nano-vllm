# Nano-vLLM

A lightweight vLLM implementation built from scratch.

## Key Features

* ğŸš€ **Fast offline inference** - Comparable inference speeds to vLLM
* ğŸ“– **Readable codebase** - Clean implementation in ~ 1,200 lines of Python code
* âš¡ **Optimization Suite** - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.
* ğŸ¯ **MoE Support** - Mixtral 8x7B with dynamic expert loading
* ğŸ”§ **Multi-GPU Architecture Support** - Flash Attention 1.x for Turing (2080 Ti), 2.x for Ampere+

## Installation

```bash
pip install git+https://github.com/GeeeekExplorer/nano-vllm.git
```

## Manual Download

If you prefer to download the model weights manually, use the following command:
```bash
huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False
```

## Quick Start

See `example.py` for usage. The API mirrors vLLM's interface with minor differences in the `LLM.generate` method:
```python
from nanovllm import LLM, SamplingParams
llm = LLM("/YOUR/MODEL/PATH", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]
```

---

## ğŸ¯ åœ¨ RTX 2080 Ti ä¸Šè¿è¡Œ Mixtral 8x7B

### 1. ç¯å¢ƒå‡†å¤‡

```bash
cd /home/asu/Desktop/nano-vllm

# åˆ›å»º2080 Tiä¸“ç”¨ç¯å¢ƒ
cd envs/2080ti
uv sync

# å®‰è£… Flash Attention 1.x (æ”¯æŒTuringæ¶æ„)
uv pip install flash-attn==1.0.9 --no-build-isolation -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 2. ä¸‹è½½ Mixtral æ¨¡å‹

```bash
# ä½¿ç”¨ huggingface-cli
huggingface-cli download --resume-download mistralai/Mixtral-8x7B-v0.1 \
  --local-dir ./Mixtral-8x7B-v0.1/ \
  --local-dir-use-symlinks False

# æˆ–ä½¿ç”¨ hfd.sh (æ¨èï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ )
./hfd.sh mistralai/Mixtral-8x7B-v0.1 --local-dir ./Mixtral-8x7B-v0.1/
```

### 3. è¿è¡Œ Mixtral

```bash
# æŒ‡å®šä½¿ç”¨ 2080 Ti (PyTorch GPU 1)
CUDA_VISIBLE_DEVICES=1 uv run python example_mixtral.py
```

æˆ–è€…ä½¿ç”¨Python APIï¼š

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # é€‰æ‹©2080 Ti

from nanovllm.config import Config
from nanovllm.engine.model_runner import ModelRunner

# é…ç½®
config = Config(
    model="/path/to/Mixtral-8x7B-v0.1",
    tensor_parallel_size=1,
    max_num_batched_tokens=2048,
    max_model_len=1024,
    gpu_memory_utilization=0.85,
    enforce_eager=True,  # 2080 Tiå»ºè®®å¼€å¯
)

# åˆå§‹åŒ–
runner = ModelRunner(config, rank=0, event=None)

# ç”Ÿæˆ...
```

### 4. æ€§èƒ½ä¼˜åŒ–å»ºè®®

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `max_model_len` | 512-1024 | å‡å°‘KV cacheå ç”¨ |
| `max_num_batched_tokens` | 1024-2048 | æ§åˆ¶æ˜¾å­˜å³°å€¼ |
| `gpu_memory_utilization` | 0.85 | ç•™å‡ºç©ºé—´ç»™æ¿€æ´»å€¼ |
| `enforce_eager` | True | 2080 Tiå»ºè®®ç¦ç”¨CUDA Graph |

### 5. éªŒè¯ Flash Attention

```bash
CUDA_VISIBLE_DEVICES=1 uv run python -c "
from nanovllm.layers.attention import print_attention_backend_info
print_attention_backend_info()
"
```

é¢„æœŸè¾“å‡ºï¼š
```
GPU: NVIDIA GeForce RTX 2080 Ti (compute 7.5)
GPUæ¶æ„: Turing (RTX 20ç³»åˆ—, æ”¯æŒFlash Attn 1.x)
Flash Attention 1.x: âœ“ å¯ç”¨
```

### 6. Attention æ€§èƒ½å¯¹æ¯”

åœ¨ RTX 2080 Ti ä¸Šçš„ Prefill æ€§èƒ½ï¼š

| seq_len | PyTorch SDPA | Flash Attn 1.x | æå‡ |
|---------|-------------|----------------|------|
| 128 | 0.77ms | 0.16ms | **4.8x** |
| 256 | 0.73ms | 0.17ms | **4.3x** |
| 512 | 0.81ms | 0.35ms | **2.3x** |
| 1024 | 1.10ms | 0.80ms | **1.4x** |

---

## Benchmark

See `bench.py` for benchmark.

**Test Configuration:**
- Hardware: RTX 4070 Laptop (8GB)
- Model: Qwen3-0.6B
- Total Requests: 256 sequences
- Input Length: Randomly sampled between 100â€“1024 tokens
- Output Length: Randomly sampled between 100â€“1024 tokens

**Performance Results:**
| Inference Engine | Output Tokens | Time (s) | Throughput (tokens/s) |
|----------------|-------------|----------|-----------------------|
| vLLM           | 133,966     | 98.37    | 1361.84               |
| Nano-vLLM      | 133,966     | 93.41    | 1434.13               |


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=GeeeekExplorer/nano-vllm&type=Date)](https://www.star-history.com/#GeeeekExplorer/nano-vllm&Date)


## SVD Expert Decomposition

Nano-vLLM æ”¯æŒä¸¤ç§ä¸“å®¶åˆ†è§£æ–¹æ³•ï¼š

### æ–¹æ³• 1: PCA åˆ†è§£ï¼ˆå¿«é€Ÿä½†å¯èƒ½äº§ç”Ÿä¹±ç ï¼‰

ä½¿ç”¨ PCA è®¡ç®—å…±äº« U çŸ©é˜µï¼Œç„¶åé€šè¿‡ `V = U^T @ W^T` è®¡ç®— Vï¼š

```bash
CUDA_VISIBLE_DEVICES=0 uv run python scripts/decompose_experts.py \
    --model-path ./Mixtral-8x7B-v0.1 \
    --rank 256 \
    --dtype float16
```

**å‚æ•°è¯´æ˜ï¼š**
- `--model-path`: Mixtral æ¨¡å‹è·¯å¾„
- `--rank`: åˆ†è§£çš„ç§©ï¼ˆé»˜è®¤ 256ï¼Œè¶Šå°å‹ç¼©ç‡è¶Šé«˜ä½†ç²¾åº¦è¶Šä½ï¼‰
- `--dtype`: è¾“å‡ºæ•°æ®ç±»å‹ï¼ˆfloat16/bfloat16/float32ï¼‰
- `--device`: è®¡ç®—è®¾å¤‡ï¼ˆcuda/cpuï¼‰

### æ–¹æ³• 2: Activation-Aware è’¸é¦ï¼ˆæ¨èï¼Œè§£å†³ä¹±ç é—®é¢˜ï¼‰

ä½¿ç”¨çœŸå®æ¿€æ´»æ•°æ®è¿›è¡Œè’¸é¦å›å½’æ‹Ÿåˆ Vï¼Œè§£å†³ PCA æ–¹æ³•å¯èƒ½å¯¼è‡´çš„è¾“å‡ºä¹±ç é—®é¢˜ã€‚

#### æ­¥éª¤ 1: é‡‡é›†æ ¡å‡†æ¿€æ´»æ•°æ®

è¿è¡Œ teacher æ¨¡å‹ï¼ˆç¦ç”¨ SVDï¼‰ï¼Œæ”¶é›†æ¯å±‚æ¯ä¸“å®¶çš„è¾“å…¥æ¿€æ´»ï¼š

```bash
CUDA_VISIBLE_DEVICES=1 uv run python scripts/collect_moe_calib.py \
    --model-path ./Mixtral-8x7B-v0.1 \
    --out ./calib_mixtral.pt \
    --cap-per-group 1024 \
    --num-prompts 200 \
    --max-new-tokens 64 \
    --temperature 0.7
```

**å‚æ•°è¯´æ˜ï¼š**
- `--model-path`: Mixtral æ¨¡å‹è·¯å¾„
- `--out`: æ ¡å‡†æ•°æ®ä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤ `calib_mixtral.pt`ï¼‰
- `--cap-per-group`: æ¯ä¸ª (layer, expert) ç»„ä¿ç•™çš„æœ€å¤§æ ·æœ¬æ•°ï¼ˆé»˜è®¤ 1024ï¼‰
- `--num-prompts`: ç”¨äºæ ¡å‡†çš„ prompt æ•°é‡ï¼ˆé»˜è®¤ 200ï¼‰
- `--max-new-tokens`: æ¯ä¸ª prompt ç”Ÿæˆçš„æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ 64ï¼‰
- `--temperature`: é‡‡æ ·æ¸©åº¦ï¼ˆé»˜è®¤ 0.7ï¼‰

**æ³¨æ„äº‹é¡¹ï¼š**
- æ­¤æ­¥éª¤ä¼šè‡ªåŠ¨è®¾ç½® `NANOVLLM_DISABLE_SVD=1`ï¼Œå¼ºåˆ¶ä½¿ç”¨åŸå§‹ expert manager
- é‡‡é›†è¿‡ç¨‹ä¼šå ç”¨ GPU æ˜¾å­˜ï¼Œå»ºè®®åœ¨æ¨ç†ç©ºé—²æ—¶è¿›è¡Œ
- æ ·æœ¬æ•°é‡è¶Šå¤šï¼Œè’¸é¦æ•ˆæœè¶Šå¥½ï¼Œä½†é‡‡é›†æ—¶é—´æ›´é•¿

#### æ­¥éª¤ 2: è’¸é¦ç”Ÿæˆ SVD Experts

ä½¿ç”¨æ ¡å‡†æ•°æ®å¯¹æ¯ä¸ª expert è¿›è¡Œ activation-aware ridge regression æ‹Ÿåˆ Vï¼š

```bash
CUDA_VISIBLE_DEVICES=1 uv run python scripts/distill_experts_activation_aware.py \
    --model-path ./Mixtral-8x7B-v0.1 \
    --calib-path ./calib_mixtral.pt \
    --rank 256 \
    --dtype float16 \
    --ridge 1e-4 \
    --device cuda
```

**å‚æ•°è¯´æ˜ï¼š**
- `--model-path`: Mixtral æ¨¡å‹è·¯å¾„
- `--calib-path`: æ­¥éª¤ 1 ç”Ÿæˆçš„æ ¡å‡†æ•°æ®è·¯å¾„
- `--rank`: åˆ†è§£çš„ç§©ï¼ˆé»˜è®¤ 256ï¼Œå¿…é¡»ä¸æ¨ç†æ—¶ä¸€è‡´ï¼‰
- `--dtype`: è¾“å‡ºæ•°æ®ç±»å‹ï¼ˆé»˜è®¤ float16ï¼‰
- `--ridge`: Ridge regression æ­£åˆ™åŒ–ç³»æ•°ï¼ˆé»˜è®¤ 1e-4ï¼‰
- `--device`: è®¡ç®—è®¾å¤‡ï¼ˆcuda/cpuï¼Œæ¨è cuda åŠ é€Ÿï¼‰
- `--chunk`: w1/w3 çš„æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤ 64ï¼‰
- `--chunk-w2`: w2 çš„æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤ 16ï¼Œw2 è®¡ç®—æ›´å¤æ‚ï¼‰
- `--pca-oversample`: w2 çš„ PCA è¿‡é‡‡æ ·å‚æ•°ï¼ˆé»˜è®¤ 32ï¼‰
- `--output-dir`: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ `{model_path}/svd_experts`ï¼‰

**è¾“å‡ºç»“æ„ï¼š**
```
{model_path}/svd_experts/
â”œâ”€â”€ U_matrices.safetensors          # å…±äº« U çŸ©é˜µï¼ˆæ¯å±‚æ¯æƒé‡ç±»å‹ï¼‰
â”œâ”€â”€ V_experts/
â”‚   â”œâ”€â”€ layer_0_expert_0.safetensors
â”‚   â”œâ”€â”€ layer_0_expert_1.safetensors
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.json                   # å…ƒæ•°æ®ï¼ˆrank, dtype, ridge ç­‰ï¼‰
```

**æ€§èƒ½æç¤ºï¼š**
- ä½¿ç”¨ `--device cuda` å¯ä»¥æ˜¾è‘—åŠ é€Ÿè’¸é¦è¿‡ç¨‹ï¼ˆæ ¡å‡†æ•°æ®ä¼šåŠ è½½åˆ° GPUï¼‰
- å¦‚æœ GPU æ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥ä½¿ç”¨ `--device cpu` æˆ–å‡å°‘ `--cap-per-group`

#### æ­¥éª¤ 3: ä½¿ç”¨ SVD Experts è¿›è¡Œæ¨ç†

è’¸é¦å®Œæˆåï¼Œæ¨ç†æ—¶ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ SVD expertsï¼š

```bash
CUDA_VISIBLE_DEVICES=1 uv run python example_mixtral.py
```

**éªŒè¯ SVD æ˜¯å¦ç”Ÿæ•ˆï¼š**

è¿è¡Œæ—¶ä¼šçœ‹åˆ°æ—¥å¿—ï¼š
```
[ModelRunner] Using SVD Expert Manager from ./Mixtral-8x7B-v0.1/svd_experts
```

### ä¸¤ç§æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **PCA åˆ†è§£** | å¿«é€Ÿï¼Œæ— éœ€æ ¡å‡†æ•°æ® | å¯èƒ½äº§ç”Ÿä¹±ç /éšæœºç¬¦å· | å¿«é€Ÿæµ‹è¯•ï¼Œå¯¹ç²¾åº¦è¦æ±‚ä¸é«˜ |
| **Activation-Aware è’¸é¦** | ç²¾åº¦é«˜ï¼Œè§£å†³ä¹±ç é—®é¢˜ | éœ€è¦æ ¡å‡†æ•°æ®ï¼Œè€—æ—¶æ›´é•¿ | ç”Ÿäº§ç¯å¢ƒï¼Œè¦æ±‚é«˜è´¨é‡è¾“å‡º |

### å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆè’¸é¦åè¿˜æ˜¯å‡ºç°ä¹±ç ï¼Ÿ**
- æ£€æŸ¥ `--rank` æ˜¯å¦å¤ªå°ï¼ˆå»ºè®® â‰¥ 256ï¼‰
- å¢åŠ  `--cap-per-group` å’Œ `--num-prompts` æ”¶é›†æ›´å¤šæ ¡å‡†æ•°æ®
- è°ƒæ•´ `--ridge` å‚æ•°ï¼ˆå°è¯• 1e-5 åˆ° 1e-3ï¼‰

**Q: è’¸é¦è¿‡ç¨‹å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ**
- ä½¿ç”¨ `--device cuda` åŠ é€Ÿ
- å‡å°‘ `--cap-per-group`ï¼ˆä½†å¯èƒ½å½±å“ç²¾åº¦ï¼‰
- å‡å°‘ `--num-prompts`ï¼ˆä½†å¯èƒ½å½±å“ç²¾åº¦ï¼‰

**Q: GPU æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
- ä½¿ç”¨ `--device cpu` è¿›è¡Œè’¸é¦
- å‡å°‘ `--cap-per-group` å‚æ•°
- åœ¨é‡‡é›†æ ¡å‡†æ•°æ®æ—¶å‡å°‘ `--max-new-tokens`

**Q: å¦‚ä½•éªŒè¯è’¸é¦æ•ˆæœï¼Ÿ**
- å¯¹æ¯”è’¸é¦å‰åçš„ç”Ÿæˆæ–‡æœ¬è´¨é‡
- æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¹±ç /éšæœºç¬¦å·
- è§‚å¯Ÿç”Ÿæˆæ–‡æœ¬çš„è¯­ä¹‰è¿è´¯æ€§