# Nano-vLLM ç§»é™¤ Flash-Attention ä¾èµ–å¼€å‘è®¡åˆ’

## é¡¹ç›®èƒŒæ™¯

å½“å‰ nano-vllm é¡¹ç›®ä¾èµ– `flash-attn` åº“ã€‚ä¸ºäº†å‡å°‘ä¾èµ–å’Œæé«˜å…¼å®¹æ€§ï¼Œéœ€è¦å®Œå…¨ç§»é™¤ flash-attn ä¾èµ–ï¼Œå¹¶å‚è€ƒ sglang çš„å®ç°æ–¹å¼è¿›è¡Œä¼˜åŒ–ã€‚

## å½“å‰çŠ¶æ€åˆ†æ

### 1. Flash-Attention ä½¿ç”¨æƒ…å†µ
- **ä¾èµ–å£°æ˜**: `pyproject.toml` ç¬¬18è¡Œå£°æ˜äº† `flash-attn` ä¾èµ–
- **å¯¼å…¥è¯­å¥**: `nanovllm/layers/attention.py` ç¬¬6è¡Œå¯¼å…¥äº† `flash_attn_varlen_func, flash_attn_with_kvcache`
- **å®é™…ä½¿ç”¨**: ä»£ç ä¸­ç¬¬70è¡Œå’Œç¬¬75è¡Œç›´æ¥è°ƒç”¨åŸåº“å‡½æ•°ï¼Œ**æ²¡æœ‰è‡ªå®šä¹‰æ›¿ä»£å®ç°**

### 2. éœ€è¦å®ç°çš„æ›¿ä»£æ–¹æ¡ˆ
- **ç¼ºå¤±**: å½“å‰æ²¡æœ‰ `flash_attn_varlen_func` çš„æ›¿ä»£å®ç°
- **ç¼ºå¤±**: å½“å‰æ²¡æœ‰ `flash_attn_with_kvcache` çš„æ›¿ä»£å®ç°
- **éœ€è¦**: åŸºäº PyTorch çš„ `scaled_dot_product_attention` å®ç°æ›¿ä»£æ–¹æ¡ˆ

### 3. SGLang å‚è€ƒå®ç°
- **TorchNativeAttnBackend**: ä½¿ç”¨ PyTorch åŸç”Ÿ `scaled_dot_product_attention`
- **TritonAttnBackend**: ä½¿ç”¨ Triton è‡ªå®šä¹‰ kernel
- æ”¯æŒå¤šç§ backend åˆ‡æ¢çš„æ¶æ„è®¾è®¡

## å¼€å‘è®¡åˆ’

### é˜¶æ®µ1: å®ç°æ›¿ä»£æ–¹æ¡ˆå¹¶ç§»é™¤ Flash-Attention ä¾èµ– (2-3å¤©)

#### 1.1 ç§»é™¤ä¾èµ–å£°æ˜
- [ ] ä» `pyproject.toml` ä¸­ç§»é™¤ `flash-attn` ä¾èµ–
- [ ] æµ‹è¯•å®‰è£…è¿‡ç¨‹ç¡®ä¿æ— é”™è¯¯

#### 1.2 å®ç°è‡ªå®šä¹‰å‡½æ•°
- [ ] åœ¨ `nanovllm/layers/attention.py` ä¸­å®ç° `attn_varlen_func` å‡½æ•°
- [ ] åœ¨ `nanovllm/layers/attention.py` ä¸­å®ç° `attn_with_kvcache` å‡½æ•°  
- [ ] å‚è€ƒ SGLang çš„ `TorchNativeAttnBackend` å®ç°
- [ ] ç¡®ä¿å‡½æ•°æ¥å£ä¸åŸ flash_attn å‡½æ•°å…¼å®¹

#### 1.3 æ›´æ–°å‡½æ•°è°ƒç”¨
- [ ] å°†ç¬¬70è¡Œçš„ `flash_attn_varlen_func` æ”¹ä¸º `attn_varlen_func`
- [ ] å°†ç¬¬75è¡Œçš„ `flash_attn_with_kvcache` æ”¹ä¸º `attn_with_kvcache`

#### 1.4 ç§»é™¤å¯¼å…¥å’Œä¾èµ–
- [ ] ç§»é™¤ `nanovllm/layers/attention.py` ä¸­çš„ `from flash_attn import` å¯¼å…¥è¯­å¥
- [ ] ä» `pyproject.toml` ä¸­ç§»é™¤ `flash-attn` ä¾èµ–

#### 1.5 åŸºæœ¬åŠŸèƒ½æµ‹è¯•
- [ ] è¿è¡Œ `example.py` ç¡®ä¿åŸºæœ¬æ¨ç†åŠŸèƒ½æ­£å¸¸
- [ ] è¿è¡Œ `bench.py` è¿›è¡Œæ€§èƒ½æµ‹è¯•ï¼Œç¡®ä¿æ€§èƒ½æ— æ˜æ˜¾ä¸‹é™

### é˜¶æ®µ2: ä¼˜åŒ–å’Œæ”¹è¿› (2-3å¤©)

#### 2.1 å‚è€ƒ SGLang çš„æ¶æ„ä¼˜åŒ–
- [ ] ç ”ç©¶ SGLang çš„ `TorchNativeAttnBackend` å®ç°ç»†èŠ‚
- [ ] ä¼˜åŒ–ç°æœ‰çš„ `flash_attn_varlen_func` å®ç°ï¼š
  - æ”¹è¿› GQA (Grouped Query Attention) å¤„ç†
  - ä¼˜åŒ–å†…å­˜ä½¿ç”¨æ¨¡å¼
  - æ”¹è¿› causal mask å¤„ç†

#### 2.2 å¢å¼º KV Cache å¤„ç†
- [ ] å‚è€ƒ SGLang ä¼˜åŒ– `flash_attn_with_kvcache` å®ç°ï¼š
  - æ”¹è¿› cache æ ¼å¼å¤„ç†
  - ä¼˜åŒ– decode é˜¶æ®µæ€§èƒ½
  - å¢å¼ºå¯¹ä¸åŒè¾“å…¥æ ¼å¼çš„å…¼å®¹æ€§

#### 2.3 è€ƒè™‘ Triton Backend (å¯é€‰)
- [ ] è¯„ä¼°æ˜¯å¦éœ€è¦æ·»åŠ  Triton backend
- [ ] å¦‚æœéœ€è¦ï¼Œå‚è€ƒ SGLang çš„ `TritonAttnBackend` å®ç° Triton ç‰ˆæœ¬

### é˜¶æ®µ3: æµ‹è¯•å’ŒéªŒè¯ (2-3å¤©)

#### 3.1 å•å…ƒæµ‹è¯•
- [ ] **åŸºç¡€åŠŸèƒ½æµ‹è¯•**
  - [ ] æµ‹è¯•ä¸åŒ batch size (1, 4, 16, 32)
  - [ ] æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦ (128, 512, 2048, 4096)
  - [ ] æµ‹è¯•ä¸åŒ head é…ç½® (num_heads: 8, 16, 32; head_dim: 64, 128)
  - [ ] æµ‹è¯• GQA é…ç½® (num_kv_heads < num_heads)

- [ ] **è¾¹ç•Œæ¡ä»¶æµ‹è¯•**
  - [ ] æçŸ­åºåˆ— (seq_len = 1)
  - [ ] æé•¿åºåˆ— (æ¥è¿‘æœ€å¤§é•¿åº¦)
  - [ ] ç©ºåºåˆ—å¤„ç†
  - [ ] ä¸è§„åˆ™ batch (å˜é•¿åºåˆ—)

- [ ] **æ•°å€¼ç²¾åº¦æµ‹è¯•**
  - [ ] ä¸åŸ flash_attn è¾“å‡ºå¯¹æ¯” (å·®å¼‚ < 1e-3)
  - [ ] æ¢¯åº¦è®¡ç®—éªŒè¯
  - [ ] ä¸åŒæ•°æ®ç±»å‹ (fp16, bf16, fp32)

#### 3.2 é›†æˆæµ‹è¯•
- [ ] **å®Œæ•´æ¨¡å‹æµ‹è¯•**
  - [ ] Qwen ç³»åˆ—æ¨¡å‹ (0.6B, 1.8B, 7B)
  - [ ] Llama ç³»åˆ—æ¨¡å‹
  - [ ] å…¶ä»–æ”¯æŒçš„æ¨¡å‹æ¶æ„

- [ ] **åŠŸèƒ½ç»„åˆæµ‹è¯•**
  - [ ] Tensor Parallelism + è‡ªå®šä¹‰ attention
  - [ ] Prefix Caching + è‡ªå®šä¹‰ attention  
  - [ ] CUDA Graph + è‡ªå®šä¹‰ attention
  - [ ] Torch Compilation + è‡ªå®šä¹‰ attention

#### 3.3 æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] **ååé‡æµ‹è¯•**
  ```bash
  # æµ‹è¯•é…ç½®
  - ç¡¬ä»¶: RTX 4070 Laptop (8GB)
  - æ¨¡å‹: Qwen3-0.6B
  - è¯·æ±‚æ•°: 256 sequences
  - è¾“å…¥é•¿åº¦: 100-1024 tokens (éšæœº)
  - è¾“å‡ºé•¿åº¦: 100-1024 tokens (éšæœº)
  ```

- [ ] **å»¶è¿Ÿæµ‹è¯•**
  - [ ] é¦–ä¸ª token å»¶è¿Ÿ (TTFT)
  - [ ] å¹³å‡ token é—´éš” (ITL)
  - [ ] ç«¯åˆ°ç«¯å»¶è¿Ÿ

- [ ] **å†…å­˜ä½¿ç”¨æµ‹è¯•**
  - [ ] å³°å€¼å†…å­˜ä½¿ç”¨
  - [ ] å†…å­˜æ³„æ¼æ£€æµ‹
  - [ ] ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„å†…å­˜scaling

#### 3.4 å¯¹æ¯”éªŒè¯
- [ ] **ä¸ Flash-Attention å¯¹æ¯”**
  - [ ] æ€§èƒ½å¯¹æ¯” (ç›®æ ‡: ä¸‹é™ < 10%)
  - [ ] å†…å­˜ä½¿ç”¨å¯¹æ¯”
  - [ ] æ•°å€¼ç²¾åº¦å¯¹æ¯”

- [ ] **ä¸ vLLM å¯¹æ¯”**
  - [ ] ç›¸åŒé…ç½®ä¸‹çš„æ€§èƒ½å¯¹æ¯”
  - [ ] èµ„æºä½¿ç”¨å¯¹æ¯”

#### 3.5 å‹åŠ›æµ‹è¯•
- [ ] **é•¿æ—¶é—´è¿è¡Œæµ‹è¯•**
  - [ ] è¿ç»­è¿è¡Œ 24 å°æ—¶
  - [ ] ç›‘æ§å†…å­˜æ³„æ¼
  - [ ] æ€§èƒ½ç¨³å®šæ€§æ£€æŸ¥

- [ ] **é«˜è´Ÿè½½æµ‹è¯•**
  - [ ] å¹¶å‘è¯·æ±‚å¤„ç†
  - [ ] èµ„æºç«äº‰åœºæ™¯
  - [ ] å¼‚å¸¸æ¢å¤æµ‹è¯•

### é˜¶æ®µ4: æ–‡æ¡£å’Œæ¸…ç† (1-2å¤©)

#### 4.1 æ›´æ–°æ–‡æ¡£
- [ ] **README.md æ›´æ–°**
  - [ ] ç§»é™¤ flash-attn ä¾èµ–è¯´æ˜
  - [ ] æ›´æ–°å®‰è£…è¯´æ˜ (æ›´ç®€å•çš„ä¾èµ–)
  - [ ] æ·»åŠ æ€§èƒ½å¯¹æ¯”æ•°æ®è¡¨æ ¼
  - [ ] è¯´æ˜å…¼å®¹æ€§æ”¹è¿›

- [ ] **æŠ€æœ¯æ–‡æ¡£**
  - [ ] æ·»åŠ  `docs/attention_implementation.md`
  - [ ] è®°å½•å®ç°ç»†èŠ‚å’Œè®¾è®¡å†³ç­–
  - [ ] æ·»åŠ  API æ–‡æ¡£
  - [ ] æ€§èƒ½è°ƒä¼˜æŒ‡å—

- [ ] **æ›´æ–°æ—¥å¿—**
  - [ ] è¯¦ç»†è®°å½•å˜æ›´å†…å®¹
  - [ ] æ ‡æ˜ç ´åæ€§å˜æ›´ (å¦‚æœæœ‰)
  - [ ] è¿ç§»æŒ‡å—

#### 4.2 ä»£ç è´¨é‡
- [ ] **ä»£ç æ¸…ç†**
  - [ ] ç§»é™¤è°ƒè¯•ä»£ç å’Œæ³¨é‡Š
  - [ ] ç»Ÿä¸€ä»£ç é£æ ¼ (black + isort)
  - [ ] æ·»åŠ å®Œæ•´çš„ç±»å‹æ³¨è§£
  - [ ] æ·»åŠ  docstring

- [ ] **æµ‹è¯•è¦†ç›–ç‡**
  - [ ] ç¡®ä¿æ ¸å¿ƒå‡½æ•° 100% è¦†ç›–
  - [ ] æ·»åŠ å¿…è¦çš„å•å…ƒæµ‹è¯•
  - [ ] é›†æˆæµ‹è¯•è¦†ç›–æ‰€æœ‰åŠŸèƒ½

#### 4.3 å‘å¸ƒå‡†å¤‡
- [ ] **ç‰ˆæœ¬ç®¡ç†**
  - [ ] æ›´æ–°ç‰ˆæœ¬å· (è€ƒè™‘ä½¿ç”¨è¯­ä¹‰åŒ–ç‰ˆæœ¬)
  - [ ] å‡†å¤‡å‘å¸ƒè¯´æ˜
  - [ ] æ ‡è®°é‡è¦çš„ git tag

- [ ] **CI/CD æ›´æ–°**
  - [ ] æ›´æ–°æµ‹è¯•æµæ°´çº¿
  - [ ] æ·»åŠ æ€§èƒ½å›å½’æµ‹è¯•
  - [ ] æ›´æ–°æ„å»ºé…ç½®

## æŠ€æœ¯å®ç°ç»†èŠ‚

### æ ¸å¿ƒä¿®æ”¹ç‚¹

1. **pyproject.toml**
```toml
# ç§»é™¤è¿™ä¸€è¡Œ
"flash-attn",
```

2. **nanovllm/layers/attention.py**
```python
# ç¬¬ä¸€æ­¥ï¼šæ·»åŠ è‡ªå®šä¹‰å®ç°å‡½æ•°ï¼ˆåœ¨å¯¼å…¥éƒ¨åˆ†ä¹‹åï¼‰
def attn_varlen_func(q, k, v, max_seqlen_q=None, cu_seqlens_q=None,
                    max_seqlen_k=None, cu_seqlens_k=None, 
                    softmax_scale=1.0, causal=True, block_table=None):
    """
    PyTorch native implementation of variable-length attention
    ä½¿ç”¨ PyTorch scaled_dot_product_attention å®ç°ï¼Œå‚è€ƒ SGLang TorchNativeAttnBackend
    """
    pass

def attn_with_kvcache(q, k_cache, v_cache, cache_seqlens=None,
                     block_table=None, softmax_scale=1.0, causal=True):
    """
    PyTorch native implementation of attention with KV cache
    ä½¿ç”¨ PyTorch scaled_dot_product_attention å®ç° KV cache ç‰ˆæœ¬
    """
    pass

# ç¬¬äºŒæ­¥ï¼šæ›´æ–°å‡½æ•°è°ƒç”¨
# ç¬¬70è¡Œ: flash_attn_varlen_func -> attn_varlen_func
# ç¬¬75è¡Œ: flash_attn_with_kvcache -> attn_with_kvcache

# ç¬¬ä¸‰æ­¥ï¼šç§»é™¤å¯¼å…¥è¯­å¥
# from flash_attn import flash_attn_varlen_func, flash_attn_with_kvcache
```

### å…³é”®æŠ€æœ¯å®ç°ç»†èŠ‚

#### å‡½æ•°ç­¾ååˆ†æ
```python
# åŸå§‹ flash_attn å‡½æ•°ç­¾å
flash_attn_varlen_func(
    q, k, v,                    # [total_tokens, num_heads, head_dim]
    cu_seqlens_q,              # [batch_size + 1] cumulative sequence lengths
    cu_seqlens_k,              # [batch_size + 1] 
    max_seqlen_q,              # int
    max_seqlen_k,              # int
    dropout_p=0.0,
    softmax_scale=None,
    causal=False,
    window_size=(-1, -1),
    alibi_slopes=None,
    return_attn_probs=False,
    block_table=None           # for prefix caching
)

flash_attn_with_kvcache(
    q,                         # [batch_size, seqlen_q, num_heads, head_dim]
    k_cache,                   # [batch_size, seqlen_k, num_kv_heads, head_dim]
    v_cache,                   # [batch_size, seqlen_k, num_kv_heads, head_dim]
    k=None,                    # new key (optional)
    v=None,                    # new value (optional)
    rotary_cos=None,
    rotary_sin=None,
    cache_seqlens=None,        # [batch_size] current cache lengths
    block_table=None,          # for paged attention
    softmax_scale=None,
    causal=False,
    window_size=(-1, -1),
    rotary_interleaved=True,
    alibi_slopes=None
)
```

#### åŸºäº SGLang çš„å®ç°ç­–ç•¥

##### 1. attn_varlen_func å®ç°
```python
def attn_varlen_func(q, k, v, cu_seqlens_q, cu_seqlens_k, 
                    max_seqlen_q, max_seqlen_k, 
                    softmax_scale=None, causal=False, **kwargs):
    """
    å‚è€ƒ SGLang TorchNativeAttnBackend._run_sdpa_forward_extend
    """
    # 1. è§£æå˜é•¿åºåˆ—
    batch_size = cu_seqlens_q.numel() - 1
    
    # 2. é€åºåˆ—å¤„ç†ï¼ˆç±»ä¼¼ SGLang çš„å¾ªç¯ï¼‰
    outputs = []
    for seq_idx in range(batch_size):
        start_q = cu_seqlens_q[seq_idx]
        end_q = cu_seqlens_q[seq_idx + 1]
        start_k = cu_seqlens_k[seq_idx] 
        end_k = cu_seqlens_k[seq_idx + 1]
        
        q_seq = q[start_q:end_q]  # [seq_len_q, num_heads, head_dim]
        k_seq = k[start_k:end_k]  # [seq_len_k, num_kv_heads, head_dim]
        v_seq = v[start_k:end_k]  # [seq_len_k, num_kv_heads, head_dim]
        
        # 3. ä½¿ç”¨ PyTorch SDPA
        output_seq = torch.nn.functional.scaled_dot_product_attention(
            q_seq.unsqueeze(0).transpose(1, 2),  # [1, num_heads, seq_len_q, head_dim]
            k_seq.unsqueeze(0).transpose(1, 2),  # [1, num_kv_heads, seq_len_k, head_dim]
            v_seq.unsqueeze(0).transpose(1, 2),  # [1, num_kv_heads, seq_len_k, head_dim]
            scale=softmax_scale,
            is_causal=causal
        ).transpose(1, 2).squeeze(0)  # [seq_len_q, num_heads, head_dim]
        
        outputs.append(output_seq)
    
    return torch.cat(outputs, dim=0)
```

##### 2. attn_with_kvcache å®ç°
```python
def attn_with_kvcache(q, k_cache, v_cache, cache_seqlens=None,
                     softmax_scale=None, causal=False, **kwargs):
    """
    å‚è€ƒ SGLang TorchNativeAttnBackend._run_sdpa_forward_decode
    """
    # 1. å¤„ç†æ‰¹æ¬¡ç»´åº¦
    if q.dim() == 3:  # [batch, num_heads, head_dim] -> [batch, 1, num_heads, head_dim]
        q = q.unsqueeze(1)
        
    batch_size = q.shape[0]
    outputs = []
    
    # 2. é€æ‰¹æ¬¡å¤„ç†
    for batch_idx in range(batch_size):
        if cache_seqlens is not None:
            cache_len = cache_seqlens[batch_idx]
            k_seq = k_cache[batch_idx, :cache_len]  # [cache_len, num_kv_heads, head_dim]
            v_seq = v_cache[batch_idx, :cache_len]  # [cache_len, num_kv_heads, head_dim]
        else:
            k_seq = k_cache[batch_idx]
            v_seq = v_cache[batch_idx]
            
        q_seq = q[batch_idx]  # [1, num_heads, head_dim]
        
        # 3. ä½¿ç”¨ PyTorch SDPA
        output_seq = torch.nn.functional.scaled_dot_product_attention(
            q_seq.unsqueeze(0).transpose(1, 2),  # [1, num_heads, 1, head_dim]
            k_seq.unsqueeze(0).transpose(1, 2),  # [1, num_kv_heads, cache_len, head_dim]
            v_seq.unsqueeze(0).transpose(1, 2),  # [1, num_kv_heads, cache_len, head_dim]
            scale=softmax_scale,
            is_causal=False  # decode é˜¶æ®µé€šå¸¸ä¸éœ€è¦ causal
        ).transpose(1, 2).squeeze(0)  # [1, num_heads, head_dim]
        
        outputs.append(output_seq)
    
    return torch.stack(outputs, dim=0)
```

### æ½œåœ¨ä¼˜åŒ–æ–¹å‘

1. **å‚è€ƒ SGLang çš„åˆ†æ‰¹å¤„ç†é€»è¾‘**
   - æ”¹è¿›åºåˆ—å¹¶è¡Œå¤„ç†
   - ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
   - æ”¯æŒ Grouped Query Attention (GQA)

2. **å¢å¼ºé”™è¯¯å¤„ç†**
   - æ·»åŠ æ›´å¥½çš„å¼‚å¸¸å¤„ç†å’Œå›é€€æœºåˆ¶
   - æä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
   - æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼çš„è‡ªåŠ¨è½¬æ¢

3. **æ€§èƒ½è°ƒä¼˜**
   - è°ƒæ•´åˆ†å—å¤§å° (chunk_size)
   - ä¼˜åŒ– CUDA å†…æ ¸é€‰æ‹©ç­–ç•¥
   - æ”¯æŒ mixed precision

## é£é™©è¯„ä¼°ä¸ç¼“è§£ç­–ç•¥

### é«˜é£é™© ğŸ”´

#### 1. æ€§èƒ½ä¸‹é™é£é™©
- **é£é™©**: PyTorch SDPA å¯èƒ½æ¯” Flash-Attention æ…¢ 10-30%
- **å½±å“**: ç”¨æˆ·ä½“éªŒä¸‹é™ï¼Œç«äº‰åŠ›å‰Šå¼±
- **ç¼“è§£ç­–ç•¥**:
  - [ ] åˆ†é˜¶æ®µå‘å¸ƒï¼Œå…ˆåœ¨æµ‹è¯•åˆ†æ”¯éªŒè¯
  - [ ] å»ºç«‹æ€§èƒ½åŸºå‡†ï¼Œè®¾ç½® 10% ä¸‹é™çš„çº¢çº¿
  - [ ] å¦‚è¶…è¿‡çº¢çº¿ï¼Œè€ƒè™‘ Triton è‡ªå®šä¹‰å®ç°
  - [ ] ä¿æŒ flash-attn ä½œä¸ºå¯é€‰ä¾èµ– (ç¯å¢ƒå˜é‡æ§åˆ¶)

#### 2. æ•°å€¼ç²¾åº¦å·®å¼‚
- **é£é™©**: ä¸åŒ attention å®ç°å¯èƒ½äº§ç”Ÿä¸åŒçš„ç»“æœ
- **å½±å“**: æ¨¡å‹è¾“å‡ºä¸ä¸€è‡´ï¼Œå½±å“å¤ç°æ€§
- **ç¼“è§£ç­–ç•¥**:
  - [ ] è®¾ç½®ä¸¥æ ¼çš„æ•°å€¼å¯¹æ¯”æµ‹è¯• (è¯¯å·® < 1e-3)
  - [ ] å¤šæ¨¡å‹éªŒè¯ï¼Œç¡®ä¿è¾“å‡ºè´¨é‡
  - [ ] æä¾›ç²¾åº¦å¯¹æ¯”æŠ¥å‘Š
  - [ ] å¦‚å‘ç°æ˜¾è‘—å·®å¼‚ï¼Œä¼˜å…ˆä¿®å¤è€Œéå‘å¸ƒ

### ä¸­ç­‰é£é™© ğŸŸ¡

#### 3. å†…å­˜ä½¿ç”¨å¢åŠ 
- **é£é™©**: è‡ªå®šä¹‰å®ç°å¯èƒ½å†…å­˜æ•ˆç‡è¾ƒä½
- **å½±å“**: åœ¨å†…å­˜å—é™ç¯å¢ƒä¸‹æ€§èƒ½ä¸‹é™
- **ç¼“è§£ç­–ç•¥**:
  - [ ] å®ç°åˆ†å—è®¡ç®—ï¼Œæ§åˆ¶å†…å­˜å³°å€¼
  - [ ] å†…å­˜ä½¿ç”¨ç›‘æ§å’ŒåŸºå‡†æµ‹è¯•
  - [ ] ä¼˜åŒ–å¼ é‡æ“ä½œï¼Œå‡å°‘ä¸´æ—¶å†…å­˜åˆ†é…

#### 4. å…¼å®¹æ€§å›å½’
- **é£é™©**: æ–°å®ç°å¯èƒ½åœ¨æŸäº›é…ç½®ä¸‹å¤±æ•ˆ
- **å½±å“**: éƒ¨åˆ†ç”¨æˆ·æ— æ³•æ­£å¸¸ä½¿ç”¨
- **ç¼“è§£ç­–ç•¥**:
  - [ ] å…¨é¢çš„é…ç½®çŸ©é˜µæµ‹è¯•
  - [ ] æ¸è¿›å¼å‘å¸ƒ (beta â†’ stable)
  - [ ] å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé™çº§æœºåˆ¶

#### 5. ç»´æŠ¤è´Ÿæ‹…å¢åŠ 
- **é£é™©**: è‡ªç»´æŠ¤ attention å®ç°éœ€è¦æ›´å¤šèµ„æº
- **å½±å“**: å¼€å‘èµ„æºåˆ†æ•£ï¼Œbug ä¿®å¤å‘¨æœŸé•¿
- **ç¼“è§£ç­–ç•¥**:
  - [ ] å®Œå–„çš„å•å…ƒæµ‹è¯•å’Œæ–‡æ¡£
  - [ ] ç®€åŒ–å®ç°ï¼Œå‡å°‘å¤æ‚åº¦
  - [ ] å»ºç«‹ç¤¾åŒºè´¡çŒ®æœºåˆ¶

### ä½é£é™© ğŸŸ¢

#### 6. å®‰è£…å¤æ‚æ€§
- **é£é™©**: ç”¨æˆ·å®‰è£…è¿‡ç¨‹å¯èƒ½å‡ºç°é—®é¢˜
- **å½±å“**: æ–°ç”¨æˆ·ä½“éªŒä¸‹é™
- **ç¼“è§£ç­–ç•¥**:
  - [ ] ä¾èµ–å‡å°‘å®é™…ä¸Šé™ä½å®‰è£…éš¾åº¦
  - [ ] æ›´æ–°å®‰è£…æ–‡æ¡£å’Œå¸¸è§é—®é¢˜è§£ç­”
  - [ ] æä¾›å¤šç§å®‰è£…æ–¹å¼

## æˆåŠŸæ ‡å‡†

1. **åŠŸèƒ½æ€§**: æ‰€æœ‰ç°æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ
2. **æ€§èƒ½**: æ€§èƒ½ä¸‹é™ä¸è¶…è¿‡ 10%
3. **å…¼å®¹æ€§**: æ”¯æŒæ‰€æœ‰ç°æœ‰çš„æ¨¡å‹å’Œé…ç½®
4. **ç¨³å®šæ€§**: é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
5. **æ˜“ç”¨æ€§**: å®‰è£…å’Œä½¿ç”¨æ›´åŠ ç®€å•

## å‘å¸ƒå’Œç›‘æ§ç­–ç•¥

### å‘å¸ƒç­–ç•¥
#### Phase 1: å†…éƒ¨éªŒè¯ (1-2å¤©)
- [ ] å¼€å‘åˆ†æ”¯å®ŒæˆåŸºæœ¬å®ç°
- [ ] å†…éƒ¨åŠŸèƒ½æµ‹è¯•é€šè¿‡
- [ ] åŸºç¡€æ€§èƒ½æµ‹è¯•è¾¾æ ‡

#### Phase 2: Beta æµ‹è¯• (3-5å¤©)  
- [ ] åˆ›å»º `beta-no-flash-attn` åˆ†æ”¯
- [ ] é‚€è¯·æ ¸å¿ƒç”¨æˆ·æµ‹è¯•
- [ ] æ”¶é›†åé¦ˆå¹¶ä¿®å¤é—®é¢˜
- [ ] æ€§èƒ½å’Œç¨³å®šæ€§éªŒè¯

#### Phase 3: æ­£å¼å‘å¸ƒ (1å¤©)
- [ ] åˆå¹¶åˆ°ä¸»åˆ†æ”¯
- [ ] å‘å¸ƒæ–°ç‰ˆæœ¬ (å¦‚ v0.3.0)
- [ ] æ›´æ–°æ–‡æ¡£å’Œè¯´æ˜

### ç›‘æ§æŒ‡æ ‡
- [ ] **æ€§èƒ½ç›‘æ§**
  - ååé‡ (tokens/second)
  - å»¶è¿ŸæŒ‡æ ‡ (TTFT, ITL)
  - å†…å­˜ä½¿ç”¨å³°å€¼

- [ ] **è´¨é‡ç›‘æ§**  
  - é”™è¯¯ç‡å’Œå´©æºƒç»Ÿè®¡
  - æ•°å€¼ç²¾åº¦å¯¹æ¯”
  - ç”¨æˆ·åé¦ˆè¯„åˆ†

- [ ] **å…¼å®¹æ€§ç›‘æ§**
  - ä¸åŒæ¨¡å‹çš„å…¼å®¹æ€§æµ‹è¯•
  - å„ç§é…ç½®ç»„åˆæµ‹è¯•
  - è¾¹ç•Œæƒ…å†µå¤„ç†

### å›é€€ç­–ç•¥
- [ ] **ç´§æ€¥å›é€€æœºåˆ¶**
  - ä¿ç•™ flash-attn ç‰ˆæœ¬çš„ git tag
  - ç¯å¢ƒå˜é‡æ§åˆ¶åˆ‡æ¢ (`USE_FLASH_ATTN=1`)
  - å¿«é€Ÿç‰ˆæœ¬å›é€€èƒ½åŠ›

- [ ] **æ¸è¿›å›é€€**
  - é¦–å…ˆç¦ç”¨æ–°åŠŸèƒ½
  - ç„¶åé™çº§åˆ°å®‰å…¨ç‰ˆæœ¬
  - æœ€ååˆ†æå’Œä¿®å¤é—®é¢˜

## æ—¶é—´ä¼°ç®—

- **æ€»è®¡**: 7-12 ä¸ªå·¥ä½œæ—¥
- **æœ€å°å¯è¡Œç‰ˆæœ¬**: 3-4 å¤© (å®ç°æ›¿ä»£å‡½æ•°ï¼Œç§»é™¤ä¾èµ–ï¼ŒåŸºæœ¬æµ‹è¯•)
- **å®Œæ•´ä¼˜åŒ–ç‰ˆæœ¬**: 7-12 å¤© (åŒ…å«æ‰€æœ‰ä¼˜åŒ–ã€æµ‹è¯•å’Œå‘å¸ƒ)

### è¯¦ç»†æ—¶é—´åˆ†é…
- **é˜¶æ®µ1** (å®ç°å’ŒåŸºæœ¬æµ‹è¯•): 2-3 å¤©
- **é˜¶æ®µ2** (ä¼˜åŒ–æ”¹è¿›): 2-3 å¤©  
- **é˜¶æ®µ3** (å…¨é¢æµ‹è¯•): 2-3 å¤©
- **é˜¶æ®µ4** (æ–‡æ¡£å’Œå‘å¸ƒ): 1-2 å¤©
- **ç¼“å†²æ—¶é—´** (é—®é¢˜ä¿®å¤): 1-2 å¤©

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### å³æ—¶è¡ŒåŠ¨ (ä»Šå¤©)
1. **ç¯å¢ƒå‡†å¤‡**: è®¾ç½®å¼€å‘åˆ†æ”¯ï¼Œç¡®ä¿ç°æœ‰åŠŸèƒ½æ­£å¸¸
2. **å‡½æ•°ç­¾ååˆ†æ**: è¯¦ç»†åˆ†æç°æœ‰è°ƒç”¨ï¼Œäº†è§£å‚æ•°æ ¼å¼
3. **SGLang ä»£ç ç ”ç©¶**: æ·±å…¥ç ”ç©¶ TorchNativeAttnBackend å®ç°

### ç¬¬ä¸€å‘¨è¡ŒåŠ¨
1. **å®ç°æ›¿ä»£å‡½æ•°**: å®Œæˆ `attn_varlen_func` å’Œ `attn_with_kvcache`
2. **åŸºç¡€æµ‹è¯•**: ç¡®ä¿æ•°å€¼æ­£ç¡®æ€§å’ŒåŸºæœ¬åŠŸèƒ½
3. **æ€§èƒ½åˆæµ‹**: äº†è§£æ€§èƒ½å·®è·ï¼Œå†³å®šæ˜¯å¦éœ€è¦ä¼˜åŒ–

### ç¬¬äºŒå‘¨è¡ŒåŠ¨  
1. **å…¨é¢æµ‹è¯•**: æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶
2. **æ€§èƒ½ä¼˜åŒ–**: æ ¹æ®æµ‹è¯•ç»“æœè¿›è¡Œå¿…è¦ä¼˜åŒ–
3. **æ–‡æ¡£æ›´æ–°**: å‡†å¤‡å‘å¸ƒç›¸å…³æ–‡æ¡£

## é•¿æœŸè€ƒè™‘

### ç¤¾åŒºå»ºè®¾
- [ ] å»ºç«‹è´¡çŒ®è€…æŒ‡å—
- [ ] è®¾ç½® issue æ¨¡æ¿å’Œ PR æ¨¡æ¿
- [ ] åˆ›å»ºæ€§èƒ½å›å½’æµ‹è¯•çš„ CI/CD

### æŠ€æœ¯æ¼”è¿›
- [ ] **å¯é€‰çš„ Flash-Attention æ”¯æŒ**
  - ç¯å¢ƒå˜é‡æ§åˆ¶: `USE_FLASH_ATTN=1`
  - è¿è¡Œæ—¶æ£€æµ‹å’Œè‡ªåŠ¨é€‰æ‹©
  - æ€§èƒ½å¯¹æ¯”å·¥å…·

- [ ] **æœªæ¥ä¼˜åŒ–æ–¹å‘**
  - Triton è‡ªå®šä¹‰ kernel (å¦‚æœæ€§èƒ½ä¸è¾¾æ ‡)
  - æ›´å¤š backend æ”¯æŒ (CUTLASS, TensorRT)
  - ç¡¬ä»¶ç‰¹å®šä¼˜åŒ– (ä¸åŒ GPU æ¶æ„)

### ç»´æŠ¤ç­–ç•¥
- [ ] **ç‰ˆæœ¬ç­–ç•¥**
  - ä¸»ç‰ˆæœ¬: é‡å¤§æ¶æ„å˜æ›´
  - æ¬¡ç‰ˆæœ¬: æ–°åŠŸèƒ½å’Œä¼˜åŒ–
  - è¡¥ä¸ç‰ˆæœ¬: Bug ä¿®å¤

- [ ] **å‘åå…¼å®¹æ€§**
  - API ç¨³å®šæ€§ä¿è¯
  - åºŸå¼ƒåŠŸèƒ½çš„è¿ç§»è·¯å¾„
  - ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯

## é¡¹ç›®æ„ä¹‰

è¿™ä¸ªé¡¹ç›®çš„æˆåŠŸå°†ä¸º nano-vLLM å¸¦æ¥ï¼š

1. **ğŸ”“ é™ä½ä½¿ç”¨é—¨æ§›**: å‡å°‘ä¾èµ–ï¼Œç®€åŒ–å®‰è£…
2. **ğŸ¯ æé«˜å…¼å®¹æ€§**: æ”¯æŒæ›´å¤šç¯å¢ƒå’Œç¡¬ä»¶
3. **ğŸ› ï¸ å¢å¼ºå¯æ§æ€§**: è‡ªä¸»æŒæ§æ ¸å¿ƒ attention å®ç°
4. **ğŸ“ˆ æŠ€æœ¯ç§¯ç´¯**: æ·±å…¥ç†è§£ attention æœºåˆ¶ï¼Œä¸ºæœªæ¥ä¼˜åŒ–æ‰“åŸºç¡€
5. **ğŸŒŸ ç¤¾åŒºä»·å€¼**: ä¸ºå¼€æºç¤¾åŒºæä¾›ä¸€ä¸ªé«˜è´¨é‡çš„ flash-attn æ›¿ä»£æ–¹æ¡ˆ

---

**æ€»ç»“**: è¿™æ˜¯ä¸€ä¸ªæŠ€æœ¯æŒ‘æˆ˜ä¸æœºé‡å¹¶å­˜çš„é¡¹ç›®ã€‚é€šè¿‡ç³»ç»Ÿçš„è®¡åˆ’ã€å……åˆ†çš„æµ‹è¯•å’Œè°¨æ…çš„å‘å¸ƒç­–ç•¥ï¼Œæˆ‘ä»¬å¯ä»¥æˆåŠŸç§»é™¤ flash-attn ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡é¡¹ç›®çš„è´¨é‡å’Œæ€§èƒ½ã€‚ 